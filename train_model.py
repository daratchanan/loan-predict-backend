# train_model.py
import pandas as pd
import numpy as np
import joblib
import os
from datetime import datetime
from dotenv import load_dotenv
# from app import db_models
from sqlalchemy import create_engine, Column, Integer, String, DateTime, Numeric, Boolean
from sqlalchemy.dialects.postgresql import JSONB
from sqlalchemy.orm import sessionmaker
from sqlalchemy.ext.declarative import declarative_base

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.multiclass import OneVsRestClassifier
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTENC
from imblearn.pipeline import Pipeline as ImblearnPipeline
from sklearn.pipeline import Pipeline
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report, 
                            roc_curve, RocCurveDisplay, roc_auc_score, precision_recall_curve, PrecisionRecallDisplay,
                            precision_score, recall_score, f1_score)

print("--- ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á, ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô, ‡πÅ‡∏•‡∏∞‡∏ù‡∏∂‡∏Å‡∏™‡∏≠‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢ ---")

# --- 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
print("Step 1: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")
load_dotenv()
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_NAME = os.getenv("DB_NAME")
DB_HOST = os.getenv("DB_HOST")
DB_PORT = os.getenv("DB_PORT")

DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"
engine = create_engine(DATABASE_URL)

try:
    df = pd.read_csv("data/loan_data.csv")
    print(f"‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô {len(df)} ‡πÅ‡∏ñ‡∏ß")
except Exception as e:
    print(f"‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {e}")
    exit()

# --- 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Preparation) ---
print("Step 2: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")

# ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç 1: ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å '.' ‡πÄ‡∏õ‡πá‡∏ô '_'
df.columns = df.columns.str.replace('.', '_', regex=False)

df_clean = df.copy()

print('Create Financial Ratios ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏á‡∏¥‡∏ô 3 ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå')
print('1. ‡∏ß‡∏á‡πÄ‡∏á‡∏¥‡∏ô‡∏™‡∏¥‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏î‡∏¢‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì estimated_credit_limit')

if 'revol_bal' in df_clean.columns and 'revol_util' in df_clean.columns:
    df_clean['estimated_credit_limit'] = df_clean['revol_bal'] / (df_clean['revol_util'] + 0.001)

print('2. ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏á‡∏ß‡∏î‡∏ï‡πà‡∏≠‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ')
# ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ log_annual_inc ‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡∏õ‡∏µ‡∏à‡∏£‡∏¥‡∏á‡πÜ" ‡∏Å‡πà‡∏≠‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå (‡∏Ñ‡πà‡∏≤ log ‡∏Ñ‡∏∑‡∏≠ ‡πÅ‡∏õ‡∏•‡∏á scale ‡πÅ‡∏•‡πâ‡∏ß)
df_clean['annual_inc'] = np.exp(df_clean['log_annual_inc'])

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á "‡∏Ñ‡πà‡∏≤‡∏á‡∏ß‡∏î‡∏ï‡πà‡∏≠‡∏õ‡∏µ" ‡∏Å‡∏±‡∏ö "‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏ï‡πà‡∏≠‡∏õ‡∏µ"
if 'installment' in df_clean.columns and 'annual_inc' in df_clean.columns:
    df_clean['installment_to_income_ratio'] = (df_clean['installment'] * 12) / df_clean['annual_inc']

df_clean.drop('annual_inc', axis=1, inplace=True)

print('3. ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏î‡∏≠‡∏Å‡πÄ‡∏ö‡∏µ‡πâ‡∏¢‡∏™‡∏π‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà')

if 'int_rate' in df_clean.columns:
    df_clean['high_interest'] = (df_clean['int_rate'] > df_clean['int_rate'].median()).astype(int)


print('4. ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå purpose (Categorical) ‡πÄ‡∏õ‡πá‡∏ô One-Hot Encoding')

df_clean = pd.get_dummies(df_clean, columns=['purpose'], drop_first=True)

print("‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
print(df_clean.head())


random = 42
target = 'not_fully_paid'

# --- 3. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Features (X) ‡πÅ‡∏•‡∏∞ Target (y) ---
print("Step 3: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î Features ‡πÅ‡∏•‡∏∞ Target...")

X = df_clean.drop(columns=target)
y = df_clean[target]

# ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå "‡πÄ‡∏ä‡∏¥‡∏á‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SMOTENC
cat_cols = [c for c in X.columns if X[c].dtype == "bool"]


# --- 4. ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train ‡πÅ‡∏•‡∏∞ Test Set ---
print("Step 4: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train (80%) ‡πÅ‡∏•‡∏∞ Test (20%) Set...")
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random, stratify=y)

# --- 5. Oversampling ‡∏î‡πâ‡∏ß‡∏¢ SMOTE ‡πÉ‡∏ä‡πâ SMOTENC ---
print("Step 5: Oversampling ‡∏î‡πâ‡∏ß‡∏¢ SMOTE ‡πÉ‡∏ä‡πâ SMOTENC...")

# 5.1 ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå bool ‡πÄ‡∏õ‡πá‡∏ô int 
X_train_int = X_train.assign(**{c: X_train[c].astype(int) for c in cat_cols})

X_test_int = X_test.assign(**{c: X_test[c].astype(int) for c in cat_cols})

# 5.2 ‡∏´‡∏≤‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà 
categorical_indices = [X_train_int.columns.get_loc(c) for c in cat_cols]

# 5.3 ‡∏õ‡∏£‡∏±‡∏ö k_neighbors ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ 
minority_count = y_train.value_counts().min()
k_neighbors = 5 if minority_count > 5 else max(1, minority_count - 1)

# 5.4 ‡∏ó‡∏≥ SMOTENC ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡πÉ‡∏´‡∏°‡πà
smote = SMOTENC(
    categorical_features=categorical_indices,
    random_state=random,
    k_neighbors=k_neighbors
)
X_train_sm, y_train_sm = smote.fit_resample(X_train_int, y_train)


# --- 6. Stacking Ensemble and Pipeling ---
print("Step 6: Stacking Ensemble and Pipeling...")

# num_cols ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏´‡∏±‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ
num_cols = [col for col in X_train_sm.columns if col not in cat_cols]

# 6.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á Preprocessor
preprocessor = ColumnTransformer(
    transformers=[('num', StandardScaler(), num_cols)],
    remainder='passthrough'  # ‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå bool ‡∏ó‡∏µ‡πà‡∏ó‡∏≥ get_dummies ‡πÅ‡∏•‡πâ‡∏ß‡∏ú‡πà‡∏≤‡∏ô‡πÑ‡∏õ‡πÄ‡∏•‡∏¢
)

# 6.2 ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Base Models ‡πÅ‡∏•‡∏∞ Meta Model ---
# Base Models
rf = RandomForestClassifier(n_estimators=400, n_jobs=-1, random_state=random)
svm = SVC(kernel="rbf", probability=True, C=2.0, gamma="scale", random_state=random)
et = ExtraTreesClassifier(n_estimators=600, n_jobs=-1, random_state=random)

# Meta Model (XGBoost) 
meta_xgb_wrapped = OneVsRestClassifier(
    XGBClassifier(
        n_estimators=250,
        max_depth=3,
        learning_rate=0.05,
        subsample=0.8,
        colsample_bytree=0.8,
        reg_lambda=1.0,
        reg_alpha=0.0,
        min_child_weight=1.0,
        objective="binary:logistic",
        eval_metric="logloss",
        n_jobs=-1,
        random_state=random
    )
)

# 6.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á StackingClassifier ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Final Estimator ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏´‡πà‡∏≠‡∏´‡∏∏‡πâ‡∏°‡πÅ‡∏•‡πâ‡∏ß 

stack_xgbmeta = StackingClassifier(
    estimators=[
        ("rf", rf),
        ("svm", svm),
        ("et", et),
    ],
    final_estimator=meta_xgb_wrapped, # <-- ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏´‡πà‡∏≠‡πÅ‡∏•‡πâ‡∏ß
    stack_method="auto",
    passthrough=False,
    cv=5,
    n_jobs=1
)

# 6.4 ‡∏£‡∏ß‡∏°‡∏ó‡∏∏‡∏Å‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÉ‡∏ô Pipeline 
model_pipeline = Pipeline(steps=[
    ("prep", preprocessor),
    ("stack", stack_xgbmeta),
])

# --- 7. ‡πÄ‡∏£‡∏¥‡πà‡∏° Train ‡πÇ‡∏°‡πÄ‡∏î‡∏• ---
print("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏° Train Stacking Ensemble Model...")

model_pipeline.fit(X_train_sm, y_train_sm)
print("‚úÖ Train ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

# --- 8. ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ Test Set ---
print("Step 8: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡πâ‡∏ß‡∏¢ Test Set...")

# 8.1 ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô (Probability) ‡∏ö‡∏ô Test Set ---
proba_test = model_pipeline.predict_proba(X_test_int)[:, 1]

# 8.2 ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ Threshold ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å F1-Score 
thresholds = np.linspace(0.00, 1.00, 101)
f1_scores = [f1_score(y_test, (proba_test >= t).astype(int), zero_division=0) for t in thresholds]

# ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤ F1 ‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡πÅ‡∏•‡∏∞ Threshold ‡∏ì ‡∏à‡∏∏‡∏î‡∏ô‡∏±‡πâ‡∏ô
best_f1_index = np.argmax(f1_scores)
best_threshold = thresholds[best_f1_index]
best_f1 = f1_scores[best_f1_index]

# 8.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á Prediction ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Threshold ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î 
y_pred_best = (proba_test >= best_threshold).astype(int)

# 8.4 ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡πÅ‡∏•‡∏∞‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ ---
accuracy = accuracy_score(y_test, y_pred_best)
cm = confusion_matrix(y_test, y_pred_best)
class_report = classification_report(y_test, y_pred_best)

print("="*60)
print("         ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (Optimal Threshold)")
print("="*60)
print(f'threshold: {best_threshold:.2f}')
print(f'Accuracy: {accuracy * 100:.4f}%')
print(cm)
print(class_report)
print("="*60)

# --- 9. ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Model Pipeline ---
print("Step 9: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å Model Pipeline ‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå joblib...")

output_dir = 'app/ml'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

joblib.dump(model_pipeline, f'{output_dir}/model_pipeline.joblib')

print(f"‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ñ‡∏π‡∏Å‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß‡∏ó‡∏µ‡πà: {output_dir}/model_pipeline.joblib")


# --- Step 10: ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏•‡∏á Database ---
print("Step 10: ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏•‡∏á‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...")

# 10.1 ‡∏Å‡∏≥‡∏´‡∏ô‡∏î ORM Model ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏≤‡∏£‡∏≤‡∏á model_performance_logs
Base = declarative_base()

class ModelPerformanceLog(Base):
    __tablename__ = 'model_performance_logs'
    id = Column(Integer, primary_key=True)
    model_version = Column(String(50))
    training_date = Column(DateTime, default=datetime.now)
    accuracy = Column(Numeric)
    precision_class_1 = Column(Numeric)
    recall_class_1 = Column(Numeric)
    f1_score_class_1 = Column(Numeric)
    is_active = Column(Boolean, default=False)
    feature_importances = Column(JSONB)

# 10.2 ‡∏™‡∏£‡πâ‡∏≤‡∏á Session ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏¢‡∏Å‡∏±‡∏ö Database
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
db = SessionLocal()

try:
    # 10.3 ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Å‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ is_active = false
    print("   - ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Å‡πà‡∏≤...")
    db.query(ModelPerformanceLog).update({ModelPerformanceLog.is_active: False})
    db.commit()

    # 10.4 ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á version ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏≤‡∏Å‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
    model_version_str = f"v{datetime.now().strftime('%Y%m%d%H%M%S')}"

    # ‡∏î‡∏∂‡∏á‡∏Ñ‡πà‡∏≤ precision, recall, f1-score ‡∏Ç‡∏≠‡∏á class 1 (fully_paid) ‡∏à‡∏≤‡∏Å classification_report
    report_dict = classification_report(y_test, y_pred_best, output_dict=True)
    class_1_metrics = report_dict.get('1', {}) # ‡πÉ‡∏ä‡πâ .get ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏´‡∏≤‡∏Å class 1 ‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏ô report

    # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: feature_importances ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Stacking Model ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
    # ‡πÅ‡∏•‡∏∞‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏° ‡∏à‡∏∂‡∏á‡∏Ç‡∏≠‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÉ‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ (‡πÉ‡∏™‡πà‡πÄ‡∏õ‡πá‡∏ô None)
    
    # 10.5 ‡∏™‡∏£‡πâ‡∏≤‡∏á Log object ‡πÉ‡∏´‡∏°‡πà
    new_log = ModelPerformanceLog(
        model_version=model_version_str,
        training_date=datetime.now(),
        accuracy=accuracy,
        precision_class_1=class_1_metrics.get('precision'),
        recall_class_1=class_1_metrics.get('recall'),
        f1_score_class_1=class_1_metrics.get('f1-score'),
        is_active=True,
        feature_importances=None 
    )

    # 10.6 ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÅ‡∏•‡∏∞ commit ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà
    db.add(new_log)
    db.commit()
    db.refresh(new_log)
    print(f"   - ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô '{model_version_str}' ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")

except Exception as e:
    print(f"   - ‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á Database: {e}")
    db.rollback()
finally:
    db.close()


print("--- ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô ---")

